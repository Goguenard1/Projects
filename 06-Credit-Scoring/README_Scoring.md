# Credit Scoring Model: Logistic Regression with WOE Transformation

A comprehensive credit risk assessment system implementing Weight of Evidence (WOE) transformation and logistic regression to build an optimal credit scoring model for loan default prediction.

## üìä Project Overview

This project develops a **credit scorecard** using advanced statistical techniques to evaluate customer default risk. The methodology follows industry best practices for credit scoring, incorporating variable selection based on Information Value (IV), optimal binning, and economic impact analysis.

**Academic Context**: Scoring - M2 IREF  
**Professor**: Monsieur Marie  
**Team**: Manel Amrani, Mohamed Baji, Enora Friant, Youness Hida

## üéØ Objectives

1. **Build a robust scorecard** with maximum 6 variables normalized to 500 points
2. **Optimize predictive power** through WOE transformation and optimal binning
3. **Ensure model interpretability** for credit decision-making
4. **Maximize economic value** by identifying optimal decision thresholds
5. **Comply with regulatory standards** for credit risk modeling

## üî¨ Methodology

### Phase 1: Data Preparation & Splitting

**Stratified Split Strategy:**
- **Training set**: 60% - Model development
- **Validation set**: 20% - Hyperparameter tuning
- **Test set**: 20% - Final performance evaluation

**Stratification**: Ensures balanced distribution of good/bad payers across all sets

### Phase 2: Variable Selection via Information Value (IV)

#### Weight of Evidence (WOE) Calculation

For each category of a variable:

```
WOE = ln(% of Good Payers / % of Bad Payers)
```

**Interpretation:**
- **WOE > 0**: Category has more good payers (lower risk)
- **WOE < 0**: Category has more bad payers (higher risk)
- **WOE ‚âà 0**: Category is neutral

#### Information Value (IV) Metric

```
IV = Œ£ (% Good - % Bad) √ó WOE
```

**IV Selection Criteria:**

| IV Range | Predictive Power | Decision |
|----------|------------------|----------|
| < 0.02 | Not useful | ‚ùå Exclude |
| 0.02 - 0.10 | Weak | ‚ö†Ô∏è Consider |
| 0.10 - 0.30 | Medium | ‚úÖ Include |
| 0.30 - 0.50 | Strong | ‚úÖ‚úÖ Priority |
| > 0.50 | Suspicious | ‚ö†Ô∏è Verify (possible overfitting) |

#### Variable Selection Results

**Categorical Variables Selected:**

| Variable | IV Score | Predictive Power | Status |
|----------|----------|------------------|--------|
| **SECTEUR_EMPLOI** | 0.0820 | Moderate-Strong | ‚úÖ Selected |
| **TYPE_REVENUS** | 0.0615 | Moderate | ‚úÖ Selected |
| **PROFESSION** | 0.0499 | Moderate | ‚úÖ Selected |
| **NIVEAU_ACADEMIQUE** | 0.0488 | Moderate | ‚úÖ Selected |
| **GENRE** | 0.0384 | Weak-Moderate | ‚úÖ Selected |
| **STATUT_FAMILIAL** | 0.0227 | Weak | ‚úÖ Selected |

**Variables Excluded:**

| Variable | IV Score | Reason for Exclusion |
|----------|----------|----------------------|
| SITUATION_HABITAT | 0.0156 | Too weak predictive power |
| TYPE_CONTRAT | 0.0148 | Insufficient discrimination |
| POSSEDE_VOITURE | 0.0076 | Negligible predictive value |
| PROPRIETAIRE | 0.0004 | No predictive value |

### Phase 3: Category Grouping & Optimization

#### Rare Category Management

**Problem**: Categories with <5% frequency cause instability

**Solution**: Strategic merging based on:
1. **Similar risk profiles** (comparable WOE values)
2. **Business logic** (meaningful combinations)
3. **Sample size** (minimum 5% threshold)

**Example - PROFESSION variable:**
- Merged rare categories (Businessman, IT staff, HR staff) with "Commercial associate"
- Rationale: Similar income variability and employment stability
- Impact: IV decreased 16.6% (0.0488 ‚Üí 0.0407) but gained model robustness

**Quality Control:**
- Accept merging if IV loss < 20%
- Ensure interpretability is maintained
- Validate business logic

### Phase 4: Optimal Binning for Numerical Variables

#### OptimalBinning Algorithm

**Objective**: Discretize continuous variables to maximize predictive power

**Constraints:**
- Maximum bins: 5-7 (for interpretability)
- Minimum bin population: 5%
- Monotonic relationship: WOE increases or decreases consistently
- Business logic: Meaningful cut-points

**Optimization Criterion**: Maximize GINI coefficient

**Process:**
1. Initialize with quantile-based bins
2. Iteratively merge adjacent bins to maximize GINI
3. Check monotonicity constraint
4. Validate minimum population per bin
5. Generate WOE mapping for final bins

**Benefits:**
- Handles non-linear relationships
- Reduces impact of outliers
- Creates interpretable risk categories
- Prevents overfitting

### Phase 5: Logistic Regression Model

#### Model Specification

```python
# After WOE transformation
P(Default = 1 | X) = 1 / (1 + exp(-(Œ≤‚ÇÄ + Œ£Œ≤·µ¢ √ó WOE·µ¢)))
```

**Feature Engineering:**
- All categorical variables transformed to WOE
- Numerical variables binned and transformed to WOE
- Maximum 6 variables in final model

**Model Training:**
- Optimization: Maximum likelihood estimation (MLE)
- Regularization: None (interpretability priority)
- Convergence: BFGS algorithm
- Class balancing: Weighted by inverse frequency

#### Variable Selection Process

**Step 1**: Start with all IV > 0.02 variables  
**Step 2**: Train logistic regression  
**Step 3**: Remove variables with p-value > 0.05  
**Step 4**: Check VIF (Variance Inflation Factor) for multicollinearity  
**Step 5**: Validate on validation set  
**Step 6**: Iterate until optimal model

**Final Model Criteria:**
- Maximum 6 variables
- All p-values < 0.05
- VIF < 5 for all variables
- GINI > 0.30 on validation set

### Phase 6: Scorecard Construction

#### Points Allocation Formula

```
Points = (Œ≤ √ó WOE + Œ±/n) √ó Factor + Offset/n
```

**Where:**
- Œ≤ = Logistic regression coefficient
- WOE = Weight of Evidence for the category
- n = Number of variables in model
- Factor = Scaling factor (typically 20-30)
- Offset = Base points (to ensure positive scores)

**Normalization:**
- Total score normalized to 500 points
- Each variable contributes proportionally to its Œ≤ coefficient
- Positive WOE adds points, negative WOE subtracts points

**Score Distribution:**
- Higher score = Lower risk (good payer)
- Lower score = Higher risk (bad payer)

#### Score Interpretation Table

| Score Range | Risk Level | Default Rate | Action |
|-------------|------------|--------------|--------|
| 450-500 | Very Low | < 5% | Auto-approve |
| 400-449 | Low | 5-10% | Approve with standard terms |
| 350-399 | Medium | 10-20% | Manual review |
| 300-349 | High | 20-35% | Approve with restrictions |
| < 300 | Very High | > 35% | Reject |

## üìà Model Evaluation

### Performance Metrics

#### 1. GINI Coefficient

```
GINI = 2 √ó AUC - 1
```

**Interpretation:**
- GINI = 1: Perfect discrimination
- GINI = 0: Random model
- **Target**: GINI > 0.30 for credit scoring

#### 2. ROC Curve Analysis

**Components:**
- True Positive Rate (TPR) = Sensitivity
- False Positive Rate (FPR) = 1 - Specificity
- Plot: TPR vs FPR across all thresholds

**Key Points:**
- Diagonal = Random classifier (GINI = 0)
- Upper-left corner = Perfect classifier
- Area Under Curve (AUC) measures overall performance

#### 3. Confusion Matrix

**At optimal threshold:**

|  | Predicted Good | Predicted Bad |
|---|---------------|--------------|
| **Actual Good** | True Negatives | False Positives |
| **Actual Bad** | False Negatives | True Positives |

**Metrics Derived:**
- **Precision**: TP / (TP + FP)
- **Recall**: TP / (TP + FN)
- **F1-Score**: 2 √ó (Precision √ó Recall) / (Precision + Recall)
- **Accuracy**: (TP + TN) / Total

### Economic Impact Analysis

#### Cost-Benefit Framework

**Costs:**
- **C‚ÇÅ**: Cost of approving a bad payer (default loss)
- **C‚ÇÇ**: Cost of rejecting a good payer (opportunity loss)

**Expected Cost per Decision:**
```
Expected Cost = P(Bad|Approve) √ó C‚ÇÅ + P(Good|Reject) √ó C‚ÇÇ
```

#### Optimal Threshold Selection

**Method**: Minimize total expected cost

**Assumptions:**
- Default loss (C‚ÇÅ): 100% of loan amount
- Opportunity cost (C‚ÇÇ): 15% (profit margin on good loan)

**Process:**
1. Calculate expected cost at each threshold
2. Plot cost curve
3. Identify minimum cost threshold
4. Validate against business constraints

**Result**: Optimal threshold that maximizes bank profitability

## üõ†Ô∏è Technologies & Libraries

### Core Libraries
```python
# Data Processing
import pandas as pd
import numpy as np

# Statistical Modeling
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, roc_curve

# Optimization & Binning
from optbinning import OptimalBinning
from scipy.optimize import minimize

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Statistical Tests
from scipy.stats import chi2_contingency
from statsmodels.stats.outliers_influence import variance_inflation_factor
```

### Specialized Tools
- **OptimalBinning**: Automated optimal discretization
- **Scikit-learn**: Machine learning framework
- **Statsmodels**: Statistical testing and diagnostics

## üìä Project Structure

### Notebook Organization

```
I. Data Preparation
   1.1. Import cleaned data
   1.2. Stratified train/validation/test split
   1.3. Exploratory analysis

II. Variable Transformation
   2.1. WOE & IV calculation for categorical variables
   2.2. Category grouping for rare modalities
   2.3. Optimal binning for numerical variables
   2.4. WOE transformation application

III. Model Development
   3.1. Variable selection based on IV
   3.2. Logistic regression training
   3.3. Coefficient significance testing
   3.4. Multicollinearity check (VIF)

IV. Scorecard Construction
   4.1. Points allocation formula
   4.2. Normalization to 500 points
   4.3. Score distribution analysis

V. Model Evaluation
   5.1. GINI coefficient calculation
   5.2. ROC curve analysis
   5.3. Confusion matrix
   5.4. Performance on test set

VI. Economic Analysis
   6.1. Cost-benefit framework
   6.2. Optimal threshold identification
   6.3. Expected profit calculation
   6.4. Business recommendations
```

## üîë Key Methodological Innovations

### 1. **Systematic IV-Based Selection**
- Objective criterion for variable inclusion
- Prevents overfitting from irrelevant variables
- Ensures regulatory compliance

### 2. **Business-Driven Category Merging**
- Balances statistical power with interpretability
- Tolerates controlled IV degradation (<20%)
- Maintains domain knowledge in model

### 3. **Constrained Optimal Binning**
- Monotonicity ensures interpretability
- Minimum bin size prevents overfitting
- Maximizes GINI while respecting constraints

### 4. **Economic Threshold Optimization**
- Moves beyond statistical metrics
- Aligns with business profitability
- Accounts for asymmetric error costs

### 5. **Transparent Scorecard**
- Point allocation traceable to coefficients
- Each variable contribution explicit
- Regulatory compliance for model explainability

## üìà Results & Insights

### Variable Importance Ranking

**By IV Score (Descending):**
1. SECTEUR_EMPLOI (0.082) - Employment sector most predictive
2. TYPE_REVENUS (0.062) - Income type strongly differentiates
3. PROFESSION (0.050) - Job role matters
4. NIVEAU_ACADEMIQUE (0.049) - Education level relevant
5. GENRE (0.038) - Gender shows weak but exploitable signal
6. STATUT_FAMILIAL (0.023) - Family status provides marginal value

**Key Findings:**
- **Employment characteristics** (sector, profession, income type) are strongest predictors
- **Demographic factors** (gender, family status) provide supplementary information
- **Asset ownership** (car, house) surprisingly weak predictors (IV < 0.01)

### Model Performance

**Expected Metrics** (typical for credit scoring):
- **GINI**: 0.30-0.45 (adequate discrimination)
- **AUC**: 0.65-0.73 (acceptable predictive power)
- **Accuracy**: 70-80% (typical for imbalanced credit data)

### Economic Impact

**Optimal Threshold Analysis:**
- Threshold set to minimize: `C_default √ó P(default|approve) + C_opportunity √ó P(good|reject)`
- **Result**: Threshold balances approval rate with default risk
- **Business value**: Quantifiable improvement in portfolio profitability

## üíº Real-World Applications

### 1. **Loan Origination**
- Automated credit decisions for consumer loans
- Tiered pricing based on score
- Quick pre-qualification

### 2. **Credit Card Issuance**
- Credit limit determination
- APR rate assignment
- Marketing campaign targeting

### 3. **Portfolio Management**
- Early warning system for deteriorating accounts
- Collection prioritization
- Provisioning calculation

### 4. **Regulatory Compliance**
- Basel III capital requirements
- IFRS 9 expected credit loss
- Fair lending documentation

## üî¨ Statistical Rigor

### Validation Framework

**1. Train-Validation-Test Split**
- Prevents overfitting
- Unbiased performance estimation
- Stratification ensures representativeness

**2. Statistical Significance**
- All coefficients tested at Œ± = 0.05
- Wald test for individual significance
- Likelihood ratio test for overall fit

**3. Multicollinearity Check**
- VIF < 5 for all variables
- Ensures stable coefficient estimates
- Prevents interpretation issues

**4. Goodness-of-Fit**
- Hosmer-Lemeshow test
- Calibration plot (predicted vs observed)
- Validates model assumptions

## üìö Theoretical Foundation

### Key Concepts Implemented

**1. Weight of Evidence (WOE)**
- Introduced by Good (1950)
- Transforms categorical variables to continuous scale
- Linearizes relationship with log-odds

**2. Information Value (IV)**
- Measures variable discriminatory power
- Based on information theory (KL divergence)
- Industry standard for variable selection

**3. Optimal Binning**
- Supervised discretization
- Maximizes univariate predictive power
- Implemented via dynamic programming

**4. Logistic Regression**
- Maximum likelihood estimation
- Probabilistic interpretation
- Linear in log-odds

**5. Economic Optimization**
- Expected utility theory
- Cost-sensitive learning
- Business value maximization

## üöÄ Future Enhancements

### Model Improvements
1. **Ensemble methods**: Combine with gradient boosting
2. **Non-linear features**: Interaction terms between top variables
3. **Time-varying covariates**: Account for economic cycles
4. **Survival analysis**: Time-to-default modeling

### Technical Extensions
1. **Automated monitoring**: Detect model degradation
2. **A/B testing framework**: Compare scorecard versions
3. **API deployment**: Real-time scoring service
4. **Explainability**: SHAP values for individual predictions

### Business Applications
1. **Dynamic pricing**: Risk-based interest rates
2. **Marketing optimization**: Targeted acquisition
3. **Early warning systems**: Proactive account management
4. **Regulatory reporting**: Automated compliance dashboards

## üìä Deliverables

This project produces:
1. ‚úÖ **Trained scorecard model** with 6 optimized variables
2. ‚úÖ **WOE transformation mappings** for deployment
3. ‚úÖ **Points allocation table** for manual scoring
4. ‚úÖ **Performance benchmarks** (GINI, AUC, confusion matrix)
5. ‚úÖ **Optimal decision threshold** based on economic analysis
6. ‚úÖ **Documentation** for regulatory compliance

## üë• Team

- **Manel Amrani** - Variable selection & WOE transformation
- **Mohamed Baji** - Optimal binning & model training
- **Enora Friant** - Scorecard construction & validation
- **Youness Hida** - Economic analysis & threshold optimization

**Professor**: Monsieur Marie  
**Program**: M2 IREF - Economic Risks and Data Science

## üìÑ Files

- **AMRANI_part2_3.ipynb**: Complete scoring methodology (10,000+ lines)
- **df_clean.csv**: Pre-processed credit data (not included - confidential)
- **WOE_mappings/**: Transformation tables for deployment

## üìù Academic Context

This project demonstrates mastery of:
- **Credit risk modeling**: Industry-standard scorecard development
- **Statistical learning**: Variable selection, regularization, validation
- **Business analytics**: Cost-benefit analysis, threshold optimization
- **Regulatory compliance**: Explainable AI for finance
- **Programming**: Advanced Python for financial modeling

## üéì Learning Outcomes

### Technical Skills
‚úÖ Weight of Evidence (WOE) transformation  
‚úÖ Information Value (IV) for variable selection  
‚úÖ Optimal binning algorithms  
‚úÖ Logistic regression for credit scoring  
‚úÖ ROC/GINI analysis  
‚úÖ Economic threshold optimization  

### Domain Knowledge
‚úÖ Credit risk assessment  
‚úÖ Scorecard development methodology  
‚úÖ Regulatory requirements (Basel III, IFRS 9)  
‚úÖ Business metrics for lending decisions  

### Software Engineering
‚úÖ End-to-end ML pipeline  
‚úÖ Reproducible data science workflows  
‚úÖ Statistical validation frameworks  

---

**Note**: This project implements production-grade credit scoring methodology aligned with banking industry standards. The scorecard can be deployed for real-world credit decisions with appropriate validation and governance.

## üîë Key Takeaway

**Credit scoring requires balancing statistical power with business interpretability.** This project achieves that balance through:
- Rigorous variable selection (IV-based)
- Transparent transformation (WOE)
- Economically optimized thresholds
- Regulatory-compliant documentation

**Result**: A 500-point scorecard using 6 variables that maximizes both predictive accuracy and business value.
